{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython import display # live plotting\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"data/python.txt\", \"r\")\n",
    "files = []\n",
    "corpus_file = ''\n",
    "for line in file:\n",
    "    if \"EOF\" not in line:\n",
    "        corpus_file += line\n",
    "    else:\n",
    "        files.append(corpus_file)\n",
    "        corpus_file = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Syntax\n",
      "Last update on February 28 2020 12:16:01 (UTC/GMT +8 hours)\n",
      "Introduction\n",
      "A Python program is read by a parser. Python was designed to be a highly readable language. The syntax of the Python programming language is the set of rules which defines how a Python program will be written.\n",
      "\n",
      "Python Line Structure:\n",
      "\n",
      "A Python program is divided into a number of logical lines and every logical line is terminated by the token NEWLINE. A logical line is created from one or more physical lines.\n",
      "A line contains only spaces, tabs, formfeeds possibly a comment, is known as a blank line, and Python interpreter ignores it.\n",
      "A physical line is a sequence of characters terminated by an end-of-line sequence (in windows it is called CR LF or return followed by a linefeed and in Unix, it is called LF or linefeed). See the following example.\n",
      "\n",
      "Python Line structure\n",
      " \n",
      "\n",
      "Comments in Python:\n",
      "\n",
      "A comment begins with a hash character(#) which is not a part of the string literal and ends at the end of the physical line. All characters after the # character up to the end of the line are part of the comment and the Python interpreter ignores them. See the following example. It should be noted that Python has no multi-lines or block comments facility.\n",
      "\n",
      "Python Comment\n",
      " \n",
      "\n",
      "Joining two lines:\n",
      "\n",
      "When you want to write a long code in a single line you can break the logical line in two or more physical lines using backslash character(\\). Therefore when a physical line ends with a backslash characters(\\) and not a part of a string literal or comment then it can join another physical line. See the following example.\n",
      "\n",
      "Python lines breaking rule\n",
      " \n",
      "\n",
      "Multiple Statements on a Single Line:\n",
      "\n",
      "You can write two separate statements into a single line using a semicolon (;) character between two line.\n",
      "\n",
      "Python multiple statement into a single line\n",
      " \n",
      "\n",
      "Indentation:\n",
      "\n",
      "Python uses whitespace (spaces and tabs) to define program blocks whereas other languages like C, C++ use braces ({}) to indicate blocks of codes for class, functions or flow control. The number of whitespaces (spaces and tabs) in the indentation is not fixed, but all statements within the block must be the indented same amount. In the following program, the block statements have no indentation.\n",
      "\n",
      "Python statements without indentation\n",
      " \n",
      "\n",
      "This is a program with single space indentation.\n",
      "\n",
      "Python single space indentation\n",
      " \n",
      "\n",
      "This is a program with single tab indentation.\n",
      "\n",
      "Python single tab indentation example\n",
      " \n",
      "\n",
      "Here is an another program with an indentation of a single space + a single tab.\n",
      "\n",
      "Python single space and -tab indentation\n",
      " \n",
      "\n",
      "Python Coding Style:\n",
      "\n",
      "Use 4 spaces per indentation and no tabs.\n",
      "Do not mix tabs and spaces. Tabs create confusion and it is recommended to use only spaces.\n",
      "Maximum line length : 79 characters which help users with a small display.\n",
      "Use blank lines to separate top-level function and class definitions and single blank line to separate methods definitions inside a class and larger blocks of code inside functions.\n",
      "When possible, put inline comments (should be complete sentences).\n",
      "Use spaces around expressions and statements.\n",
      "Python Reserve words:\n",
      "\n",
      "The following identifiers are used as reserved words of the language, and cannot be used as ordinary identifiers.\n",
      "\n",
      "False\tclass\tfinally\tis\treturn\n",
      "None\tcontinue\tfor\tlambda\ttry\n",
      "True\tdef\tfrom\tnonlocal\twhile\n",
      "and\tdel\tglobal\tnot\twith\n",
      "as\tel\tif\tor\tyield\n",
      "assert\telse\timport\tpass\t \n",
      "break\texcept\tin\traise\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for Generate a tutorial given a string data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    data = data.split()\n",
    "    data = [dat.lower().strip() for dat  in data]\n",
    "    return ' '.join(data)\n",
    "\n",
    "def load_data(data):\n",
    "    data = preprocess_data('\\n'.join(data))\n",
    "    chars = tuple(set(data))\n",
    "    # Look up tables\n",
    "    idx2char = dict(enumerate(chars))\n",
    "    char2idx = {char: idx for idx, char in idx2char.items()}\n",
    "    # Encoding\n",
    "    encoded = torch.tensor([char2idx[ch] for ch in data])\n",
    "    return chars, encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \"\"\"\n",
    "    One-hot encoding for character-data\n",
    "    \"\"\"\n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "\n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "\n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    \"\"\"\n",
    "    Batch generator that returns mini-batches of size (n_seqs x n_steps)\n",
    "    \"\"\"\n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr) // batch_size\n",
    "\n",
    "    # always create full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    \n",
    "    # reshape\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # features (sequence of characters)\n",
    "        x = arr[:, n:n + n_steps]\n",
    "        \n",
    "        # targets (the next character after the sequence)\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n + n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Basic implementation of a multi-layer RNN with LSTM cells and Dropout.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers,\n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Forward pass through the network\n",
    "        \"\"\"\n",
    "\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x, hidden\n",
    "\n",
    "    def predict(self, char, hidden=None, device=torch.device('cpu'), top_k=None):\n",
    "        \"\"\"\n",
    "        Given a character, predict the next character. Returns the predicted character and the hidden state.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.to(device)\n",
    "            try:\n",
    "                x = np.array([[self.char2int[char]]])\n",
    "            except KeyError:\n",
    "                return '', hidden\n",
    "\n",
    "            x = one_hot_encode(x, len(self.chars))\n",
    "            inputs = torch.from_numpy(x).to(device)\n",
    "\n",
    "            out, hidden = self.forward(inputs, hidden)\n",
    "\n",
    "            p = F.softmax(out, dim=2).data.to('cpu')\n",
    "\n",
    "            if top_k is None:\n",
    "                top_ch = np.arange(len(self.chars))\n",
    "            else:\n",
    "                p, top_ch = p.topk(top_k)\n",
    "                top_ch = top_ch.numpy().squeeze()\n",
    "\n",
    "            if top_k == 1:\n",
    "                char = int(top_ch)\n",
    "            else:\n",
    "                p = p.numpy().squeeze()\n",
    "                char = np.random.choice(top_ch, p=p / p.sum())\n",
    "\n",
    "            return self.int2char[char], hidden\n",
    "\n",
    "\n",
    "def save_checkpoint(net, opt, filename, train_history={}):\n",
    "    \"\"\"\n",
    "    Save trained model to file.\n",
    "    \"\"\"\n",
    "    checkpoint = {'n_hidden': net.n_hidden,\n",
    "                  'n_layers': net.n_layers,\n",
    "                  'state_dict': net.state_dict(),\n",
    "                  'optimizer': opt.state_dict(),\n",
    "                  'tokens': net.chars,\n",
    "                  'train_history': train_history}\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        torch.save(checkpoint, f)\n",
    "\n",
    "\n",
    "def load_checkpoint(filename):\n",
    "    \"\"\"\n",
    "    Load trained model from file.\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        checkpoint = torch.load(f, map_location='cpu')\n",
    "\n",
    "    net = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "    net.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    return net, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion() # Allow live updates of plots\n",
    "\n",
    "def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.001, clip=5, val_frac=0.1, device=torch.device('cpu'),\n",
    "          name='checkpoint', early_stop=True, plot=True):\n",
    "    \"\"\"\n",
    "    Training loop.\n",
    "    \"\"\"\n",
    "    net.train() # switch into training mode\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr) # initialize optimizer\n",
    "    criterion = nn.CrossEntropyLoss() # initialize loss function\n",
    "\n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data) * (1 - val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    net.to(device) # move neural net to GPU/CPU memory\n",
    "    \n",
    "    min_val_loss = 10.**10 # initialize minimal validation loss\n",
    "    train_history = {'epoch': [], 'step': [], 'loss': [], 'val_loss': []}\n",
    "\n",
    "    n_chars = len(net.chars) # get size of vocabulary\n",
    "    \n",
    "    # main loop over training epochs\n",
    "    for e in range(epochs):\n",
    "        hidden = None # reste hidden state after each epoch\n",
    "        \n",
    "        # loop over batches\n",
    "        for x, y in get_batches(data, n_seqs, n_steps):\n",
    "            # encode data and create torch-tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x).to(device), torch.tensor(y, dtype=torch.long).to(device)\n",
    "            \n",
    "            # reset gradient information\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # generate network output\n",
    "            output, hidden = net.forward(inputs, hidden)\n",
    "            \n",
    "            # compute loss\n",
    "            loss = criterion(output.view(n_seqs * n_steps, n_chars), targets.view(n_seqs * n_steps))\n",
    "            \n",
    "            # compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # gradient clipping to prevent exploding gradients\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            \n",
    "            # optmize\n",
    "            opt.step()\n",
    "\n",
    "            # prevent backpropagating through the entire training history\n",
    "            # by detaching hidden state and cell state\n",
    "            hidden = (hidden[0].detach(), hidden[1].detach())\n",
    "        \n",
    "        # validation step is done without tracking gradients\n",
    "        with torch.no_grad():\n",
    "            val_h = None\n",
    "            val_losses = []\n",
    "            \n",
    "            for x, y in get_batches(val_data, n_seqs, n_steps):\n",
    "                x = one_hot_encode(x, n_chars)\n",
    "                inputs, targets = torch.from_numpy(x).to(device), torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "                output, val_h = net.forward(inputs, val_h)\n",
    "                \n",
    "                val_loss = criterion(output.view(n_seqs * n_steps, n_chars), targets.view(n_seqs * n_steps))\n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            # compute mean validation loss over batches\n",
    "            mean_val_loss = np.mean(val_losses)\n",
    "            \n",
    "            # track progress\n",
    "            train_history['epoch'].append(e+1)\n",
    "            train_history['loss'].append(loss.item())\n",
    "            train_history['val_loss'].append(mean_val_loss)\n",
    "        \n",
    "        if plot:\n",
    "            # create live plot of training loss and validation loss\n",
    "            plt.clf()\n",
    "            plt.plot(train_history['loss'], lw=2, c='C0')\n",
    "            plt.plot(train_history['val_loss'], lw=2, c='C1')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.title(\"{}   Epoch: {:.0f}/{:.0f}   Loss: {:.4f}   Val Loss: {:.4f}\".format(\n",
    "                datetime.now().strftime('%H:%M:%S'),\n",
    "                e+1, epochs,\n",
    "                loss.item(),\n",
    "                mean_val_loss), color='k')\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "        else:\n",
    "            # print training progress\n",
    "            print(\"{}   Epoch: {:.0f}/{:.0f}   Loss: {:.4f}   Val Loss: {:.4f}\".format(\n",
    "                datetime.now().strftime('%H:%M:%S'),\n",
    "                e+1, epochs,\n",
    "                loss.item(),\n",
    "                mean_val_loss))\n",
    "        \n",
    "        # save model checkpoint if validation loss has decreased\n",
    "        \n",
    "        save_checkpoint(net, opt, name+'.net', train_history=train_history)\n",
    "        min_val_loss = mean_val_loss\n",
    "        \n",
    "        # if validation loss has not decreased for the last 10 epochs, stop training\n",
    "        if early_stop:\n",
    "            if e - np.argmin(train_history['val_loss']) > 10:\n",
    "                display.clear_output()\n",
    "                print('Validation loss does not decrease further, stopping training.')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss does not decrease further, stopping training.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# load data\n",
    "chars, data = load_data(files)\n",
    "\n",
    "# create RNN\n",
    "net = CharRNN(chars, n_hidden=256, n_layers=3)\n",
    "\n",
    "# train\n",
    "plt.figure(figsize=(12, 4))\n",
    "train(net, data, epochs=500, n_seqs=40, n_steps=40, lr=0.001, device=device, val_frac=0.1,\n",
    "      name='training-blog', plot=True, early_stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def load_checkpoint(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        checkpoint = torch.load(f, map_location='cpu')\n",
    "\n",
    "    net = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "    net.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    train_history = checkpoint['train_history']\n",
    "\n",
    "    return net, train_history\n",
    "\n",
    "\n",
    "def sample_lines(net, n_lines=3, prime='', top_k=None, device='cpu', max_len=1000):\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "\n",
    "    # First off, run through the prime characters\n",
    "    chars = []\n",
    "    h = None\n",
    "    for ch in prime:\n",
    "        char, h = net.predict(ch, h, device=device, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "\n",
    "    # Now pass in the previous character and get a new one\n",
    "    l = 0\n",
    "    for ii in range(max_len):\n",
    "        char, h = net.predict(chars[-1], h, device=device, top_k=top_k)\n",
    "        chars.append(char)\n",
    "        if char == '\\n':\n",
    "            l += 1\n",
    "            if l == n_lines:\n",
    "                break\n",
    "\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "def clean_string(x):\n",
    "    x = re.sub('#.*$', '', x, flags=re.MULTILINE)\n",
    "    x = re.sub(\"'''[\\s\\S]*?'''\", '', x, flags=re.MULTILINE)\n",
    "    x = re.sub('\"\"\"[\\s\\S]*?\"\"\"', '', x, flags=re.MULTILINE)\n",
    "    x = re.sub('^[\\t]+\\n', '', x, flags=re.MULTILINE)\n",
    "    x = re.sub('^[ ]+\\n', '', x, flags=re.MULTILINE)\n",
    "    x = re.sub('\\n[\\n]+', '\\n\\n', x, flags=re.MULTILINE)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, _ = load_checkpoint('training-blog.net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ting strings. see the following example collection and a string. python dictionary in python statements in python strings are immutable to the function, a python string strings in and a collection and see the following example in the following example, a tuple >>> #create the tuple to true is an an of the sum of a see the following statements in python sets is assignment in the comples of a python dictionary in python, the sequence of the language on the string, the set of strings are supports as part of the function or the same of an operator it is an expression, as a string. in python diction\n"
     ]
    }
   ],
   "source": [
    "prime = 'python string format'\n",
    "\n",
    "clean_prime = clean_string(prime)\n",
    "print(sample_lines(net, 3, prime=clean_prime, top_k=3, max_len=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: recollect more data, change to word rnn instead of char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_regex_expression, preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9089e54f1bd649dcb36872a4d260d609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "regex = get_regex_expression()\n",
    "train_data =  [preprocess_data(file, regex) for file in tqdm(files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'syntax', 'last', 'update', 'on', 'february', 'utc', 'gmt', 'hours', 'introduction', 'python', 'program', 'is', 'read', 'by', 'parser', 'python', 'was', 'designed', 'to', 'be', 'highly', 'readable', 'language', 'the', 'syntax', 'of', 'the', 'python', 'programming', 'language', 'is', 'the', 'set', 'of', 'rules', 'which', 'defines', 'how', 'python', 'program', 'will', 'be', 'written', 'python', 'line', 'structure', 'python', 'program', 'is', 'divided', 'into', 'number', 'of', 'logical', 'lines', 'and', 'every', 'logical', 'line', 'is', 'terminated', 'by', 'the', 'token', 'newline', 'logical', 'line', 'is', 'created', 'from', 'or', 'more', 'physical', 'lines', 'line', 'contains', 'only', 'spaces', 'tabs', 'formfeeds', 'possibly', 'comment', 'is', 'known', 'as', 'blank', 'line', 'and', 'python', 'interpreter', 'ignores', 'it', 'physical', 'line', 'is', 'sequence', 'of', 'characters', 'terminated', 'by', 'an', 'end', 'of', 'line', 'sequence', 'in', 'windows', 'it', 'is', 'called', 'cr', 'lf', 'or', 'return', 'followed', 'by', 'linefeed', 'and', 'in', 'unix', 'it', 'is', 'called', 'lf', 'or', 'linefeed', 'see', 'the', 'following', 'example', 'python', 'line', 'structure', 'comments', 'in', 'python', 'comment', 'begins', 'with', 'hash', 'character', 'which', 'is', 'not', 'part', 'of', 'the', 'string', 'literal', 'and', 'ends', 'at', 'the', 'end', 'of', 'the', 'physical', 'line', 'all', 'characters', 'after', 'the', 'character', 'up', 'to', 'the', 'end', 'of', 'the', 'line', 'are', 'part', 'of', 'the', 'comment', 'and', 'the', 'python', 'interpreter', 'ignores', 'them', 'see', 'the', 'following', 'example', 'it', 'should', 'be', 'noted', 'that', 'python', 'has', 'no', 'multi', 'lines', 'or', 'block', 'comments', 'facility', 'python', 'comment', 'joining', 'lines', 'when', 'you', 'want', 'to', 'write', 'long', 'code', 'in', 'single', 'line', 'you', 'can', 'break', 'the', 'logical', 'line', 'in', 'or', 'more', 'physical', 'lines', 'using', 'backslash', 'character', 'therefore', 'when', 'physical', 'line', 'ends', 'with', 'backslash', 'characters', 'and', 'not', 'part', 'of', 'string', 'literal', 'or', 'comment', 'then', 'it', 'can', 'join', 'another', 'physical', 'line', 'see', 'the', 'following', 'example', 'python', 'lines', 'breaking', 'rule', 'multiple', 'statements', 'on', 'single', 'line', 'you', 'can', 'write', 'separate', 'statements', 'into', 'single', 'line', 'using', 'semicolon', 'character', 'between', 'line', 'python', 'multiple', 'statement', 'into', 'single', 'line', 'indentation', 'python', 'uses', 'whitespace', 'spaces', 'and', 'tabs', 'to', 'define', 'program', 'blocks', 'whereas', 'other', 'languages', 'like', 'use', 'braces', 'to', 'indicate', 'blocks', 'of', 'codes', 'for', 'class', 'functions', 'or', 'flow', 'control', 'the', 'number', 'of', 'whitespaces', 'spaces', 'and', 'tabs', 'in', 'the', 'indentation', 'is', 'not', 'fixed', 'but', 'all', 'statements', 'within', 'the', 'block', 'must', 'be', 'the', 'indented', 'same', 'amount', 'in', 'the', 'following', 'program', 'the', 'block', 'statements', 'have', 'no', 'indentation', 'python', 'statements', 'without', 'indentation', 'this', 'is', 'program', 'with', 'single', 'space', 'indentation', 'python', 'single', 'space', 'indentation', 'this', 'is', 'program', 'with', 'single', 'tab', 'indentation', 'python', 'single', 'tab', 'indentation', 'example', 'here', 'is', 'an', 'another', 'program', 'with', 'an', 'indentation', 'of', 'single', 'space', 'single', 'tab', 'python', 'single', 'space', 'and', 'tab', 'indentation', 'python', 'coding', 'style', 'use', 'spaces', 'per', 'indentation', 'and', 'no', 'tabs', 'do', 'not', 'mix', 'tabs', 'and', 'spaces', 'tabs', 'create', 'confusion', 'and', 'it', 'is', 'recommended', 'to', 'use', 'only', 'spaces', 'maximum', 'line', 'length', 'characters', 'which', 'help', 'users', 'with', 'small', 'display', 'use', 'blank', 'lines', 'to', 'separate', 'top', 'level', 'function', 'and', 'class', 'definitions', 'and', 'single', 'blank', 'line', 'to', 'separate', 'methods', 'definitions', 'inside', 'class', 'and', 'larger', 'blocks', 'of', 'code', 'inside', 'functions', 'when', 'possible', 'put', 'inline', 'comments', 'should', 'be', 'complete', 'sentences', 'use', 'spaces', 'around', 'expressions', 'and', 'statements', 'python', 'reserve', 'words', 'the', 'following', 'identifiers', 'are', 'used', 'as', 'reserved', 'words', 'of', 'the', 'language', 'and', 'can', 'not', 'be', 'used', 'as', 'ordinary', 'identifiers', 'false', 'class', 'finally', 'is', 'return', 'none', 'continue', 'for', 'lambda', 'try', 'true', 'def', 'from', 'nonlocal', 'while', 'and', 'del', 'global', 'not', 'with', 'as', 'el', 'if', 'or', 'yield', 'assert', 'else', 'import', 'pass', 'break', 'except', 'in', 'raise']\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laguage Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, Phrases\n",
    "import multiprocessing\n",
    "number_cpus = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-28 05:00:27,494 : INFO : collecting all words and their counts\n",
      "2020-06-28 05:00:27,499 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2020-06-28 05:00:27,566 : INFO : collected 9878 word types from a corpus of 16154 words (unigram + bigrams) and 19 sentences\n",
      "2020-06-28 05:00:27,570 : INFO : using 9878 counts as vocab in Phrases<0 vocab, min_count=1, threshold=5, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "bigrams = Phrases(train_data, threshold=5, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(bigrams[train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"min_count\":0, \"size\":300, \"sg\":1, \"window\":15, \"iter\":110,\n",
    "                   \"sample\": 1e-5, \"hs\": 0, \"negative\": 25, \"ns_exponent\": 0.5,\n",
    "                   \"workers\": number_cpus}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-28 05:00:29,831 : INFO : collecting all words and their counts\n",
      "2020-06-28 05:00:29,833 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-06-28 05:00:29,841 : INFO : collected 2598 word types from a corpus of 11839 raw words and 19 sentences\n",
      "2020-06-28 05:00:29,844 : INFO : Loading a fresh vocabulary\n",
      "2020-06-28 05:00:29,859 : INFO : effective_min_count=0 retains 2598 unique words (100% of original 2598, drops 0)\n",
      "2020-06-28 05:00:29,863 : INFO : effective_min_count=0 leaves 11839 word corpus (100% of original 11839, drops 0)\n",
      "2020-06-28 05:00:29,898 : INFO : deleting the raw counts dictionary of 2598 items\n",
      "2020-06-28 05:00:29,903 : INFO : sample=1e-05 downsamples 2598 most-common words\n",
      "2020-06-28 05:00:29,906 : INFO : downsampling leaves estimated 1843 word corpus (15.6% of prior 11839)\n",
      "2020-06-28 05:00:29,929 : INFO : estimated required memory for 2598 words and 300 dimensions: 7534200 bytes\n",
      "2020-06-28 05:00:29,930 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "sg_model = Word2Vec(**parameters)\n",
    "sg_model.build_vocab(train_data, progress_per=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-28 05:00:30,706 : INFO : training model with 4 workers on 2598 vocabulary and 300 features, using sg=1 hs=0 sample=1e-05 negative=25 window=15\n",
      "2020-06-28 05:00:30,719 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:30,726 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:30,829 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:30,994 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:30,994 : INFO : EPOCH - 1 : training on 11839 raw words (1824 effective words) took 0.3s, 6588 effective words/s\n",
      "2020-06-28 05:00:31,000 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:31,003 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:31,047 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:31,210 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:31,211 : INFO : EPOCH - 2 : training on 11839 raw words (1862 effective words) took 0.2s, 8839 effective words/s\n",
      "2020-06-28 05:00:31,218 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:31,226 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:31,283 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:31,500 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:31,501 : INFO : EPOCH - 3 : training on 11839 raw words (1815 effective words) took 0.3s, 6405 effective words/s\n",
      "2020-06-28 05:00:31,507 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:31,518 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:31,580 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:31,711 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:31,712 : INFO : EPOCH - 4 : training on 11839 raw words (1844 effective words) took 0.2s, 8821 effective words/s\n",
      "2020-06-28 05:00:31,719 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:31,722 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:31,788 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:31,940 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:31,941 : INFO : EPOCH - 5 : training on 11839 raw words (1862 effective words) took 0.2s, 8286 effective words/s\n",
      "2020-06-28 05:00:31,950 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:31,953 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:32,008 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:32,136 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:32,137 : INFO : EPOCH - 6 : training on 11839 raw words (1802 effective words) took 0.2s, 9471 effective words/s\n",
      "2020-06-28 05:00:32,144 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:32,147 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:32,198 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:32,368 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:32,368 : INFO : EPOCH - 7 : training on 11839 raw words (1869 effective words) took 0.2s, 8234 effective words/s\n",
      "2020-06-28 05:00:32,379 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:32,381 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:32,432 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:32,606 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:32,608 : INFO : EPOCH - 8 : training on 11839 raw words (1884 effective words) took 0.2s, 8092 effective words/s\n",
      "2020-06-28 05:00:32,618 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:32,620 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:32,676 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:32,821 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:32,822 : INFO : EPOCH - 9 : training on 11839 raw words (1807 effective words) took 0.2s, 8703 effective words/s\n",
      "2020-06-28 05:00:32,830 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:32,831 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:32,889 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:33,060 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:33,061 : INFO : EPOCH - 10 : training on 11839 raw words (1816 effective words) took 0.2s, 7716 effective words/s\n",
      "2020-06-28 05:00:33,069 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:33,073 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:33,127 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:33,318 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:33,318 : INFO : EPOCH - 11 : training on 11839 raw words (1881 effective words) took 0.3s, 7436 effective words/s\n",
      "2020-06-28 05:00:33,326 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:33,330 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:33,389 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:33,555 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:33,556 : INFO : EPOCH - 12 : training on 11839 raw words (1829 effective words) took 0.2s, 7875 effective words/s\n",
      "2020-06-28 05:00:33,564 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:33,566 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:33,613 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:33,763 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:33,764 : INFO : EPOCH - 13 : training on 11839 raw words (1853 effective words) took 0.2s, 9087 effective words/s\n",
      "2020-06-28 05:00:33,771 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:33,779 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:33,819 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:33,953 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:33,954 : INFO : EPOCH - 14 : training on 11839 raw words (1885 effective words) took 0.2s, 10167 effective words/s\n",
      "2020-06-28 05:00:33,960 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:33,971 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:34,026 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:34,213 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:34,215 : INFO : EPOCH - 15 : training on 11839 raw words (1874 effective words) took 0.3s, 7294 effective words/s\n",
      "2020-06-28 05:00:34,225 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:34,229 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:34,318 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:34,527 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:34,529 : INFO : EPOCH - 16 : training on 11839 raw words (1852 effective words) took 0.3s, 6035 effective words/s\n",
      "2020-06-28 05:00:34,550 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:34,558 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:34,648 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:34,813 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:34,814 : INFO : EPOCH - 17 : training on 11839 raw words (1874 effective words) took 0.3s, 6712 effective words/s\n",
      "2020-06-28 05:00:34,821 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:34,824 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:34,880 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:35,020 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:35,021 : INFO : EPOCH - 18 : training on 11839 raw words (1850 effective words) took 0.2s, 9129 effective words/s\n",
      "2020-06-28 05:00:35,027 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:35,038 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:35,090 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:35,235 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:35,236 : INFO : EPOCH - 19 : training on 11839 raw words (1875 effective words) took 0.2s, 8899 effective words/s\n",
      "2020-06-28 05:00:35,242 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:35,244 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:35,292 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:35,432 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:35,433 : INFO : EPOCH - 20 : training on 11839 raw words (1788 effective words) took 0.2s, 9266 effective words/s\n",
      "2020-06-28 05:00:35,439 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:35,455 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:35,544 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:35,703 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:35,703 : INFO : EPOCH - 21 : training on 11839 raw words (1870 effective words) took 0.3s, 7015 effective words/s\n",
      "2020-06-28 05:00:35,712 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:35,717 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:35,798 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:35,990 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:35,991 : INFO : EPOCH - 22 : training on 11839 raw words (1815 effective words) took 0.3s, 6453 effective words/s\n",
      "2020-06-28 05:00:35,999 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:36,002 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:36,092 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:36,300 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:36,300 : INFO : EPOCH - 23 : training on 11839 raw words (1844 effective words) took 0.3s, 6072 effective words/s\n",
      "2020-06-28 05:00:36,311 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:36,318 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:36,383 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:36,585 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:36,586 : INFO : EPOCH - 24 : training on 11839 raw words (1890 effective words) took 0.3s, 6817 effective words/s\n",
      "2020-06-28 05:00:36,593 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:36,594 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:36,662 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:36,865 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:36,865 : INFO : EPOCH - 25 : training on 11839 raw words (1844 effective words) took 0.3s, 6693 effective words/s\n",
      "2020-06-28 05:00:36,871 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:36,874 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:36,953 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:37,150 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:37,152 : INFO : EPOCH - 26 : training on 11839 raw words (1875 effective words) took 0.3s, 6641 effective words/s\n",
      "2020-06-28 05:00:37,163 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:37,167 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:37,235 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:37,403 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:37,405 : INFO : EPOCH - 27 : training on 11839 raw words (1834 effective words) took 0.2s, 7460 effective words/s\n",
      "2020-06-28 05:00:37,415 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:37,417 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:37,479 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:37,660 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:37,661 : INFO : EPOCH - 28 : training on 11839 raw words (1833 effective words) took 0.2s, 7348 effective words/s\n",
      "2020-06-28 05:00:37,667 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:37,673 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:37,749 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:37,918 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:37,918 : INFO : EPOCH - 29 : training on 11839 raw words (1837 effective words) took 0.3s, 7245 effective words/s\n",
      "2020-06-28 05:00:37,924 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:37,927 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:37,986 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:38,189 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:38,190 : INFO : EPOCH - 30 : training on 11839 raw words (1863 effective words) took 0.3s, 6910 effective words/s\n",
      "2020-06-28 05:00:38,198 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:38,204 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:38,275 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:38,435 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:38,435 : INFO : EPOCH - 31 : training on 11839 raw words (1849 effective words) took 0.2s, 7692 effective words/s\n",
      "2020-06-28 05:00:38,442 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:38,445 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:38,502 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:38,682 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:38,683 : INFO : EPOCH - 32 : training on 11839 raw words (1838 effective words) took 0.2s, 7572 effective words/s\n",
      "2020-06-28 05:00:38,689 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:38,692 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:38,762 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:38,918 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:38,918 : INFO : EPOCH - 33 : training on 11839 raw words (1844 effective words) took 0.2s, 7984 effective words/s\n",
      "2020-06-28 05:00:38,925 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:38,928 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:38,987 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:39,168 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:39,169 : INFO : EPOCH - 34 : training on 11839 raw words (1883 effective words) took 0.2s, 7661 effective words/s\n",
      "2020-06-28 05:00:39,178 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:39,185 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:39,253 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:39,395 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:39,397 : INFO : EPOCH - 35 : training on 11839 raw words (1810 effective words) took 0.2s, 8162 effective words/s\n",
      "2020-06-28 05:00:39,405 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:39,407 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:39,457 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:39,636 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:39,637 : INFO : EPOCH - 36 : training on 11839 raw words (1809 effective words) took 0.2s, 7705 effective words/s\n",
      "2020-06-28 05:00:39,644 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:39,656 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:39,718 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:39,897 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:39,898 : INFO : EPOCH - 37 : training on 11839 raw words (1887 effective words) took 0.3s, 7372 effective words/s\n",
      "2020-06-28 05:00:39,905 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:39,913 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:39,963 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:40,124 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:40,125 : INFO : EPOCH - 38 : training on 11839 raw words (1803 effective words) took 0.2s, 8029 effective words/s\n",
      "2020-06-28 05:00:40,133 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:40,135 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:40,222 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:40,393 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:40,393 : INFO : EPOCH - 39 : training on 11839 raw words (1834 effective words) took 0.3s, 7045 effective words/s\n",
      "2020-06-28 05:00:40,399 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:40,402 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:40,460 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:40,612 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:40,613 : INFO : EPOCH - 40 : training on 11839 raw words (1826 effective words) took 0.2s, 8435 effective words/s\n",
      "2020-06-28 05:00:40,621 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:40,637 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:40,684 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:40,849 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:40,850 : INFO : EPOCH - 41 : training on 11839 raw words (1826 effective words) took 0.2s, 7895 effective words/s\n",
      "2020-06-28 05:00:40,856 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:40,867 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:40,898 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:41,135 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:41,136 : INFO : EPOCH - 42 : training on 11839 raw words (1828 effective words) took 0.3s, 6487 effective words/s\n",
      "2020-06-28 05:00:41,142 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:41,144 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:41,212 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:41,351 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:41,352 : INFO : EPOCH - 43 : training on 11839 raw words (1826 effective words) took 0.2s, 8527 effective words/s\n",
      "2020-06-28 05:00:41,359 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:41,369 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:41,420 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:41,561 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:41,562 : INFO : EPOCH - 44 : training on 11839 raw words (1820 effective words) took 0.2s, 8833 effective words/s\n",
      "2020-06-28 05:00:41,569 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:41,571 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:41,616 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:41,753 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:41,754 : INFO : EPOCH - 45 : training on 11839 raw words (1801 effective words) took 0.2s, 9587 effective words/s\n",
      "2020-06-28 05:00:41,761 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:41,763 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:41,824 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:41,991 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:41,992 : INFO : EPOCH - 46 : training on 11839 raw words (1817 effective words) took 0.2s, 7796 effective words/s\n",
      "2020-06-28 05:00:41,999 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:42,000 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:42,055 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:42,180 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:42,181 : INFO : EPOCH - 47 : training on 11839 raw words (1856 effective words) took 0.2s, 10038 effective words/s\n",
      "2020-06-28 05:00:42,187 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:42,198 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:42,236 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:42,408 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:42,409 : INFO : EPOCH - 48 : training on 11839 raw words (1869 effective words) took 0.2s, 8340 effective words/s\n",
      "2020-06-28 05:00:42,424 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:42,427 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:42,492 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:42,628 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:42,629 : INFO : EPOCH - 49 : training on 11839 raw words (1823 effective words) took 0.2s, 8766 effective words/s\n",
      "2020-06-28 05:00:42,636 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:42,639 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:42,689 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:42,828 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:42,828 : INFO : EPOCH - 50 : training on 11839 raw words (1819 effective words) took 0.2s, 9339 effective words/s\n",
      "2020-06-28 05:00:42,835 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:42,836 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:42,879 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:43,035 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:43,035 : INFO : EPOCH - 51 : training on 11839 raw words (1847 effective words) took 0.2s, 9095 effective words/s\n",
      "2020-06-28 05:00:43,042 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:43,047 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:43,106 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:43,289 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:43,290 : INFO : EPOCH - 52 : training on 11839 raw words (1851 effective words) took 0.3s, 7399 effective words/s\n",
      "2020-06-28 05:00:43,299 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:43,300 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:43,387 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:43,542 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:43,542 : INFO : EPOCH - 53 : training on 11839 raw words (1861 effective words) took 0.2s, 7544 effective words/s\n",
      "2020-06-28 05:00:43,548 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:43,551 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:43,592 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:43,740 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:43,741 : INFO : EPOCH - 54 : training on 11839 raw words (1842 effective words) took 0.2s, 9471 effective words/s\n",
      "2020-06-28 05:00:43,752 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:43,753 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:43,804 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:43,999 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:44,000 : INFO : EPOCH - 55 : training on 11839 raw words (1896 effective words) took 0.3s, 7497 effective words/s\n",
      "2020-06-28 05:00:44,009 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:44,014 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:44,079 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:44,259 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:44,260 : INFO : EPOCH - 56 : training on 11839 raw words (1779 effective words) took 0.3s, 6946 effective words/s\n",
      "2020-06-28 05:00:44,266 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:44,270 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:44,335 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:44,493 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:44,494 : INFO : EPOCH - 57 : training on 11839 raw words (1839 effective words) took 0.2s, 7938 effective words/s\n",
      "2020-06-28 05:00:44,501 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:44,503 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:44,555 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:44,846 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:44,847 : INFO : EPOCH - 58 : training on 11839 raw words (1816 effective words) took 0.4s, 5180 effective words/s\n",
      "2020-06-28 05:00:44,868 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:44,870 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:44,922 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:45,093 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:45,094 : INFO : EPOCH - 59 : training on 11839 raw words (1811 effective words) took 0.2s, 7614 effective words/s\n",
      "2020-06-28 05:00:45,103 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:45,105 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:45,153 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:45,346 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:45,347 : INFO : EPOCH - 60 : training on 11839 raw words (1828 effective words) took 0.2s, 7404 effective words/s\n",
      "2020-06-28 05:00:45,359 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:45,363 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:45,459 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:45,702 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:45,702 : INFO : EPOCH - 61 : training on 11839 raw words (1837 effective words) took 0.3s, 5276 effective words/s\n",
      "2020-06-28 05:00:45,710 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:45,712 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:45,784 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:46,493 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:46,494 : INFO : EPOCH - 62 : training on 11839 raw words (1825 effective words) took 0.8s, 2322 effective words/s\n",
      "2020-06-28 05:00:46,507 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:46,508 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:46,588 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:47,351 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:47,352 : INFO : EPOCH - 63 : training on 11839 raw words (1910 effective words) took 0.8s, 2252 effective words/s\n",
      "2020-06-28 05:00:47,367 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:47,369 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:47,435 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:47,858 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:47,859 : INFO : EPOCH - 64 : training on 11839 raw words (1884 effective words) took 0.5s, 3779 effective words/s\n",
      "2020-06-28 05:00:47,874 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:47,876 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:47,952 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:48,229 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:48,231 : INFO : EPOCH - 65 : training on 11839 raw words (1900 effective words) took 0.4s, 5262 effective words/s\n",
      "2020-06-28 05:00:48,240 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:48,246 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:48,335 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:48,543 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:48,543 : INFO : EPOCH - 66 : training on 11839 raw words (1816 effective words) took 0.3s, 5941 effective words/s\n",
      "2020-06-28 05:00:48,552 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:48,555 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:48,623 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:48,770 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:48,771 : INFO : EPOCH - 67 : training on 11839 raw words (1906 effective words) took 0.2s, 8582 effective words/s\n",
      "2020-06-28 05:00:48,779 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:48,781 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:48,841 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:49,039 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:49,040 : INFO : EPOCH - 68 : training on 11839 raw words (1865 effective words) took 0.3s, 7090 effective words/s\n",
      "2020-06-28 05:00:49,049 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:49,053 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:49,154 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:49,316 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:49,317 : INFO : EPOCH - 69 : training on 11839 raw words (1862 effective words) took 0.3s, 6863 effective words/s\n",
      "2020-06-28 05:00:49,327 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:49,331 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:49,387 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:49,508 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:49,509 : INFO : EPOCH - 70 : training on 11839 raw words (1848 effective words) took 0.2s, 9972 effective words/s\n",
      "2020-06-28 05:00:49,516 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:49,517 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:49,563 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:49,755 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:49,756 : INFO : EPOCH - 71 : training on 11839 raw words (1870 effective words) took 0.2s, 7678 effective words/s\n",
      "2020-06-28 05:00:49,765 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:49,766 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:49,833 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:50,078 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:50,080 : INFO : EPOCH - 72 : training on 11839 raw words (1871 effective words) took 0.3s, 5876 effective words/s\n",
      "2020-06-28 05:00:50,094 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:50,097 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:50,208 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:50,461 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:50,463 : INFO : EPOCH - 73 : training on 11839 raw words (1834 effective words) took 0.4s, 4933 effective words/s\n",
      "2020-06-28 05:00:50,477 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:50,480 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:50,537 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:50,707 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:50,707 : INFO : EPOCH - 74 : training on 11839 raw words (1804 effective words) took 0.2s, 7638 effective words/s\n",
      "2020-06-28 05:00:50,718 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:50,721 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:50,785 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:50,996 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:50,997 : INFO : EPOCH - 75 : training on 11839 raw words (1886 effective words) took 0.3s, 6666 effective words/s\n",
      "2020-06-28 05:00:51,010 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:51,020 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:51,099 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:51,463 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:51,464 : INFO : EPOCH - 76 : training on 11839 raw words (1867 effective words) took 0.5s, 4087 effective words/s\n",
      "2020-06-28 05:00:51,476 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:51,489 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:51,560 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:52,015 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:52,015 : INFO : EPOCH - 77 : training on 11839 raw words (1826 effective words) took 0.5s, 3365 effective words/s\n",
      "2020-06-28 05:00:52,024 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:52,032 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:52,109 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:52,437 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:52,440 : INFO : EPOCH - 78 : training on 11839 raw words (1814 effective words) took 0.4s, 4335 effective words/s\n",
      "2020-06-28 05:00:52,454 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:52,457 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:52,531 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:52,671 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:52,672 : INFO : EPOCH - 79 : training on 11839 raw words (1845 effective words) took 0.2s, 8299 effective words/s\n",
      "2020-06-28 05:00:52,678 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:52,680 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:52,742 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:53,322 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:53,322 : INFO : EPOCH - 80 : training on 11839 raw words (1821 effective words) took 0.6s, 2814 effective words/s\n",
      "2020-06-28 05:00:53,338 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:53,341 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:53,399 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:53,694 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:53,695 : INFO : EPOCH - 81 : training on 11839 raw words (1847 effective words) took 0.4s, 5126 effective words/s\n",
      "2020-06-28 05:00:53,707 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:53,710 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:53,763 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:53,867 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:53,868 : INFO : EPOCH - 82 : training on 11839 raw words (1812 effective words) took 0.2s, 11011 effective words/s\n",
      "2020-06-28 05:00:53,873 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:53,876 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:53,915 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:54,032 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:54,033 : INFO : EPOCH - 83 : training on 11839 raw words (1882 effective words) took 0.2s, 11824 effective words/s\n",
      "2020-06-28 05:00:54,039 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:54,042 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:54,081 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:54,185 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:54,186 : INFO : EPOCH - 84 : training on 11839 raw words (1838 effective words) took 0.1s, 12283 effective words/s\n",
      "2020-06-28 05:00:54,194 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:54,195 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:54,238 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:54,364 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:54,365 : INFO : EPOCH - 85 : training on 11839 raw words (1797 effective words) took 0.2s, 10344 effective words/s\n",
      "2020-06-28 05:00:54,373 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:54,374 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:54,424 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:54,630 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:54,631 : INFO : EPOCH - 86 : training on 11839 raw words (1857 effective words) took 0.3s, 7128 effective words/s\n",
      "2020-06-28 05:00:54,640 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:54,642 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:54,684 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:54,813 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:54,814 : INFO : EPOCH - 87 : training on 11839 raw words (1887 effective words) took 0.2s, 10646 effective words/s\n",
      "2020-06-28 05:00:54,819 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:54,826 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:54,881 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:55,111 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:55,112 : INFO : EPOCH - 88 : training on 11839 raw words (1870 effective words) took 0.3s, 6383 effective words/s\n",
      "2020-06-28 05:00:55,125 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:55,126 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:55,192 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:55,531 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:55,532 : INFO : EPOCH - 89 : training on 11839 raw words (1850 effective words) took 0.4s, 4475 effective words/s\n",
      "2020-06-28 05:00:55,542 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:55,545 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:55,603 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:55,824 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:55,825 : INFO : EPOCH - 90 : training on 11839 raw words (1802 effective words) took 0.3s, 6295 effective words/s\n",
      "2020-06-28 05:00:55,842 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:55,846 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:55,921 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:56,303 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:56,304 : INFO : EPOCH - 91 : training on 11839 raw words (1823 effective words) took 0.5s, 3901 effective words/s\n",
      "2020-06-28 05:00:56,316 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:56,317 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:56,362 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:56,501 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:56,504 : INFO : EPOCH - 92 : training on 11839 raw words (1830 effective words) took 0.2s, 9544 effective words/s\n",
      "2020-06-28 05:00:56,518 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:56,525 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:56,577 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:56,706 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:56,707 : INFO : EPOCH - 93 : training on 11839 raw words (1894 effective words) took 0.2s, 9846 effective words/s\n",
      "2020-06-28 05:00:56,713 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:56,715 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:56,752 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:56,867 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:56,867 : INFO : EPOCH - 94 : training on 11839 raw words (1850 effective words) took 0.2s, 11810 effective words/s\n",
      "2020-06-28 05:00:56,875 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:56,877 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:56,919 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:57,065 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:57,066 : INFO : EPOCH - 95 : training on 11839 raw words (1826 effective words) took 0.2s, 9350 effective words/s\n",
      "2020-06-28 05:00:57,077 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:57,082 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:57,141 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:57,248 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:57,249 : INFO : EPOCH - 96 : training on 11839 raw words (1847 effective words) took 0.2s, 10507 effective words/s\n",
      "2020-06-28 05:00:57,255 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:57,257 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:57,296 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:57,411 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:57,412 : INFO : EPOCH - 97 : training on 11839 raw words (1843 effective words) took 0.2s, 11588 effective words/s\n",
      "2020-06-28 05:00:57,419 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:57,420 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:57,457 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:57,583 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:57,584 : INFO : EPOCH - 98 : training on 11839 raw words (1885 effective words) took 0.2s, 11196 effective words/s\n",
      "2020-06-28 05:00:57,590 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:57,592 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:57,634 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:57,754 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:57,754 : INFO : EPOCH - 99 : training on 11839 raw words (1914 effective words) took 0.2s, 11642 effective words/s\n",
      "2020-06-28 05:00:57,764 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:57,767 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:57,812 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:58,067 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:58,076 : INFO : EPOCH - 100 : training on 11839 raw words (1838 effective words) took 0.3s, 5840 effective words/s\n",
      "2020-06-28 05:00:58,100 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:58,111 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:58,215 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:58,320 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:58,321 : INFO : EPOCH - 101 : training on 11839 raw words (1746 effective words) took 0.2s, 7534 effective words/s\n",
      "2020-06-28 05:00:58,328 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:58,329 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:58,361 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:58,482 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:58,482 : INFO : EPOCH - 102 : training on 11839 raw words (1798 effective words) took 0.2s, 11469 effective words/s\n",
      "2020-06-28 05:00:58,497 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:58,499 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:58,548 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:58,670 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:58,671 : INFO : EPOCH - 103 : training on 11839 raw words (1910 effective words) took 0.2s, 10799 effective words/s\n",
      "2020-06-28 05:00:58,707 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:58,711 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:58,900 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:59,011 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:59,012 : INFO : EPOCH - 104 : training on 11839 raw words (1773 effective words) took 0.3s, 5419 effective words/s\n",
      "2020-06-28 05:00:59,020 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:59,022 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:59,057 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:59,174 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:59,175 : INFO : EPOCH - 105 : training on 11839 raw words (1876 effective words) took 0.2s, 11863 effective words/s\n",
      "2020-06-28 05:00:59,182 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:59,184 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:59,227 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:59,364 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:59,365 : INFO : EPOCH - 106 : training on 11839 raw words (1843 effective words) took 0.2s, 9908 effective words/s\n",
      "2020-06-28 05:00:59,373 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:59,376 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:59,428 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:59,552 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:59,553 : INFO : EPOCH - 107 : training on 11839 raw words (1834 effective words) took 0.2s, 10087 effective words/s\n",
      "2020-06-28 05:00:59,559 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:59,561 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:00:59,604 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:00:59,847 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:00:59,849 : INFO : EPOCH - 108 : training on 11839 raw words (1795 effective words) took 0.3s, 6134 effective words/s\n",
      "2020-06-28 05:00:59,868 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:00:59,869 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:01:00,016 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:01:00,146 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:01:00,146 : INFO : EPOCH - 109 : training on 11839 raw words (1813 effective words) took 0.3s, 6406 effective words/s\n",
      "2020-06-28 05:01:00,156 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:01:00,157 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:01:00,205 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:01:00,387 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:01:00,396 : INFO : EPOCH - 110 : training on 11839 raw words (1833 effective words) took 0.2s, 7543 effective words/s\n",
      "2020-06-28 05:01:00,400 : INFO : training on a 1302290 raw words (202793 effective words) took 29.7s, 6830 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(202793, 1302290)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model.train(train_data, total_examples=sg_model.corpus_count, \n",
    "                  epochs=sg_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('initialize', 0.9924840927124023),\n",
       " ('conforming', 0.9918240308761597),\n",
       " ('interface', 0.9900710582733154),\n",
       " ('null', 0.9896188974380493),\n",
       " ('errors', 0.988990068435669),\n",
       " ('different_ways', 0.9879573583602905),\n",
       " ('few', 0.9861245155334473),\n",
       " ('and_optionally', 0.9806928634643555),\n",
       " ('buffer', 0.966650128364563),\n",
       " ('encoding', 0.9592510461807251)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model.wv.most_similar(\"array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fse.models import SIF\n",
    "from fse import IndexedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-28 05:04:23,491 : INFO : no frequency mode: using wordfreq for estimation of frequency for language: en\n",
      "2020-06-28 05:04:23,629 : INFO : scanning all indexed sentences and their word counts\n",
      "2020-06-28 05:04:23,630 : INFO : finished scanning 19 sentences with an average length of 623 and 11839 total words\n",
      "2020-06-28 05:04:23,632 : INFO : estimated memory for 19 sentences with 300 dimensions and 2598 vocabulary: 3 MB (0 GB)\n",
      "2020-06-28 05:04:23,633 : INFO : initializing sentence vectors for 19 sentences\n",
      "2020-06-28 05:04:23,635 : INFO : pre-computing SIF weights for 2598 words\n",
      "2020-06-28 05:04:23,639 : INFO : begin training\n",
      "2020-06-28 05:04:23,646 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-28 05:04:23,648 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-28 05:04:23,649 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-28 05:04:23,650 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-28 05:04:23,652 : INFO : no removal of principal components\n",
      "2020-06-28 05:04:23,654 : INFO : training on 19 effective sentences with 11839 effective words took 0s with 1511 sentences/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19, 11839)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fse_data = IndexedList(train_data)\n",
    "fse_model = SIF(sg_model, components=0, lang_freq=\"en\", workers=number_cpus)\n",
    "fse_model.train(fse_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'ARRAYS_SORTING',\n",
       " 1: 'LOOPS_FOR',\n",
       " 2: 'LOOPS_WHILE',\n",
       " 3: 'CONDITIONALS_IF',\n",
       " 4: 'STRINGS_MANIPULATION'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = {\"ARRAYS_SORTING\": \"array list tuples sorting\",\n",
    "\"LOOPS_FOR\": \"for loop in range\",\n",
    "\"LOOPS_WHILE\": \"while loop\",\n",
    "\"CONDITIONALS_IF\": \"if else logical expresion statement conditionally\",\n",
    "\"STRINGS_MANIPULATION\": \"string formationg\"}\n",
    "db_inv = dict((idx, name) for idx, name in enumerate(db.keys()))\n",
    "db_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_data = []\n",
    "for name, data in db.items():\n",
    "    data_ = list(bigrams[preprocess_data(data,regex, False, False)])\n",
    "    db_data.append(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-28 05:20:25,474 : INFO : scanning all indexed sentences and their word counts\n",
      "2020-06-28 05:20:25,475 : INFO : finished scanning 1 sentences with an average length of 4 and 4 total words\n",
      "2020-06-28 05:20:25,478 : INFO : no removal of principal components\n",
      "2020-06-28 05:20:25,479 : INFO : scanning all indexed sentences and their word counts\n",
      "2020-06-28 05:20:25,481 : INFO : finished scanning 1 sentences with an average length of 2 and 2 total words\n",
      "2020-06-28 05:20:25,483 : INFO : no removal of principal components\n",
      "2020-06-28 05:20:25,484 : INFO : scanning all indexed sentences and their word counts\n",
      "2020-06-28 05:20:25,486 : INFO : finished scanning 1 sentences with an average length of 1 and 1 total words\n",
      "2020-06-28 05:20:25,487 : INFO : no removal of principal components\n",
      "2020-06-28 05:20:25,489 : INFO : scanning all indexed sentences and their word counts\n",
      "2020-06-28 05:20:25,492 : INFO : finished scanning 1 sentences with an average length of 5 and 5 total words\n",
      "2020-06-28 05:20:25,497 : INFO : no removal of principal components\n",
      "2020-06-28 05:20:25,498 : INFO : scanning all indexed sentences and their word counts\n",
      "2020-06-28 05:20:25,499 : INFO : finished scanning 1 sentences with an average length of 2 and 2 total words\n",
      "2020-06-28 05:20:25,500 : INFO : no removal of principal components\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.02244267, -0.00157539, -0.12970585, ...,  0.00579001,\n",
       "        -0.04598518,  0.10972005],\n",
       "       [ 0.10918188,  0.10210018,  0.10939761, ...,  0.14754297,\n",
       "        -0.21050593,  0.38786837],\n",
       "       [ 0.28281417,  0.00219268, -0.35358807, ..., -0.03955086,\n",
       "         0.06573328,  0.39092261],\n",
       "       [ 0.12153632, -0.14177927, -0.19873072, ...,  0.10271734,\n",
       "         0.10359672,  0.15887237],\n",
       "       [-0.11373401,  0.20417204, -0.1748758 , ...,  0.03863646,\n",
       "         0.03323203,  0.01003863]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_embeddings = np.zeros(shape=(len(db_data),300))\n",
    "for idx,i in enumerate(db_data):\n",
    "    data_embeddings[idx,:] = fse_model.infer([(i, 0)])\n",
    "data_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-28 05:23:32,579 : INFO : scanning all indexed sentences and their word counts\n",
      "2020-06-28 05:23:32,580 : INFO : finished scanning 1 sentences with an average length of 4 and 4 total words\n",
      "2020-06-28 05:23:32,581 : INFO : no removal of principal components\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how_to', 'do', 'string', 'format']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'STRINGS_MANIPULATION'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"how to do a string format?\"\n",
    "data = preprocess_data(s, regex, False, False)\n",
    "data_trans = list(bigrams[data])\n",
    "print(data_trans)\n",
    "query_embeddings = fse_model.infer([(data_trans,0)])\n",
    "distances = cdist(query_embeddings, data_embeddings, \"cosine\")[0]\n",
    "results = list(enumerate(distances))\n",
    "results_ = sorted(results, key=lambda x: x[1], reverse=False)\n",
    "db_inv[results_[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_data = {\"db\":db, \"db_inv\":db_inv, \"data_embeddings\":data_embeddings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bigrams, open(\"bigrams.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(fse_model, open(\"fse_model.pickle\", \"wb\"))\n",
    "pickle.dump(db_data, open(\"db_data.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat bot for simple question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import random\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "intents_file = open('data/intents.json').read()\n",
    "intents = json.loads(intents_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['Hi', 'there'], 'greeting'), (['How', 'are', 'you'], 'greeting'), (['Is', 'anyone', 'there', '?'], 'greeting'), (['Hey'], 'greeting'), (['Hola'], 'greeting'), (['Hello'], 'greeting'), (['Good', 'day'], 'greeting'), (['Bye'], 'goodbye'), (['See', 'you', 'later'], 'goodbye'), (['Goodbye'], 'goodbye'), (['Nice', 'chatting', 'to', 'you', ',', 'bye'], 'goodbye'), (['Till', 'next', 'time'], 'goodbye'), (['Thanks'], 'thanks'), (['Thank', 'you'], 'thanks'), (['That', \"'s\", 'helpful'], 'thanks'), (['Awesome', ',', 'thanks'], 'thanks'), (['Thanks', 'for', 'helping', 'me'], 'thanks'), (['How', 'you', 'could', 'help', 'me', '?'], 'options'), (['What', 'you', 'can', 'do', '?'], 'options'), (['What', 'help', 'you', 'provide', '?'], 'options'), (['How', 'you', 'can', 'be', 'helpful', '?'], 'options'), (['What', 'support', 'is', 'offered'], 'options'), (['how', 'to', 'use', 'list', 'in', 'python', '?'], 'python'), (['how', 'to', 'sort', 'lists', 'in', 'python', '?'], 'python'), (['using', 'tuples', 'in', 'python', '?'], 'python'), (['how', 'to', 'sum', 'all', 'the', 'elements', 'in', 'an', 'array', '?'], 'python')]\n"
     ]
    }
   ],
   "source": [
    "words=[]\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_letters = ['!', '?', ',', '.']\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        word = nltk.word_tokenize(pattern)\n",
    "        words.extend(word)        \n",
    "        documents.append((word, intent['tag']))\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 documents\n",
      "5 classes ['goodbye', 'greeting', 'options', 'python', 'thanks']\n",
      "53 unique lemmatized words [\"'s\", 'all', 'an', 'anyone', 'are', 'array', 'awesome', 'be', 'bye', 'can', 'chatting', 'could', 'day', 'do', 'element', 'for', 'good', 'goodbye', 'hello', 'help', 'helpful', 'helping', 'hey', 'hi', 'hola', 'how', 'in', 'is', 'later', 'list', 'me', 'next', 'nice', 'offered', 'provide', 'python', 'see', 'sort', 'sum', 'support', 'thank', 'thanks', 'that', 'the', 'there', 'till', 'time', 'to', 'tuples', 'use', 'using', 'what', 'you']\n"
     ]
    }
   ],
   "source": [
    "# lemmaztize and lower each word and remove duplicates\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_letters]\n",
    "words = sorted(list(set(words)))\n",
    "# sort classes\n",
    "classes = sorted(list(set(classes)))\n",
    "# documents = combination between patterns and intents\n",
    "print (len(documents), \"documents\")\n",
    "# classes = intents\n",
    "print (len(classes), \"classes\", classes)\n",
    "# words = all words, vocabulary\n",
    "print (len(words), \"unique lemmatized words\", words)\n",
    "pickle.dump(words,open('data/words.pkl','wb'))\n",
    "pickle.dump(classes,open('data/classes.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data is created\n"
     ]
    }
   ],
   "source": [
    "# create the training data\n",
    "training = []\n",
    "# create empty array for the output\n",
    "output_empty = [0] * len(classes)\n",
    "# training set, bag of words for every sentence\n",
    "for doc in documents:\n",
    "    # initializing bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    word_patterns = doc[0]\n",
    "    # lemmatize each word - create base word, in attempt to represent related words\n",
    "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
    "    # create the bag of words array with 1, if word is found in current pattern\n",
    "    for word in words:\n",
    "        bag.append(1) if word in word_patterns else bag.append(0)    \n",
    "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    training.append([bag, output_row])\n",
    "# shuffle the features and make numpy array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "# create training and testing lists. X - patterns, Y - intents\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])\n",
    "print(\"Training data is created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 0s 717us/step - loss: 1.6692 - accuracy: 0.2308\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 701us/step - loss: 1.5939 - accuracy: 0.2308\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 786us/step - loss: 1.5560 - accuracy: 0.3077\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 791us/step - loss: 1.5459 - accuracy: 0.2692\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 785us/step - loss: 1.4864 - accuracy: 0.2692\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 778us/step - loss: 1.3424 - accuracy: 0.3846\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 863us/step - loss: 1.3234 - accuracy: 0.5769\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 649us/step - loss: 1.3319 - accuracy: 0.4615\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.2483 - accuracy: 0.5769\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 785us/step - loss: 0.9832 - accuracy: 0.8077\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 684us/step - loss: 0.9873 - accuracy: 0.7692\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 906us/step - loss: 1.0648 - accuracy: 0.5769\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 798us/step - loss: 0.8338 - accuracy: 0.8077\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 687us/step - loss: 0.8520 - accuracy: 0.6923\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 669us/step - loss: 0.7871 - accuracy: 0.6923\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 858us/step - loss: 0.7331 - accuracy: 0.7308\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 679us/step - loss: 0.7344 - accuracy: 0.7308\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 601us/step - loss: 0.5452 - accuracy: 0.9231\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 623us/step - loss: 0.6632 - accuracy: 0.7692\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 638us/step - loss: 0.3891 - accuracy: 0.9231\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5178 - accuracy: 0.8077\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 809us/step - loss: 0.4102 - accuracy: 0.8846\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 722us/step - loss: 0.4235 - accuracy: 0.8077\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 725us/step - loss: 0.3808 - accuracy: 0.9231\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 609us/step - loss: 0.3660 - accuracy: 0.9231\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 612us/step - loss: 0.3610 - accuracy: 0.8846\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 624us/step - loss: 0.2040 - accuracy: 1.0000\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 694us/step - loss: 0.3069 - accuracy: 0.9231\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 560us/step - loss: 0.3435 - accuracy: 0.8846\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 661us/step - loss: 0.1559 - accuracy: 1.0000\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 644us/step - loss: 0.2201 - accuracy: 0.9615\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 621us/step - loss: 0.2047 - accuracy: 0.9615\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 753us/step - loss: 0.2813 - accuracy: 0.9231\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 621us/step - loss: 0.1094 - accuracy: 0.9615\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 754us/step - loss: 0.1578 - accuracy: 0.9231\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 668us/step - loss: 0.1838 - accuracy: 0.9615\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 634us/step - loss: 0.0878 - accuracy: 1.0000\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 659us/step - loss: 0.1609 - accuracy: 1.0000\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 673us/step - loss: 0.0985 - accuracy: 0.9615\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 657us/step - loss: 0.1554 - accuracy: 0.9615\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 641us/step - loss: 0.1811 - accuracy: 0.9231\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 587us/step - loss: 0.1005 - accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 645us/step - loss: 0.0856 - accuracy: 0.9615\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 703us/step - loss: 0.0721 - accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 587us/step - loss: 0.0679 - accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 611us/step - loss: 0.1716 - accuracy: 0.9231\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 610us/step - loss: 0.0905 - accuracy: 0.9615\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 597us/step - loss: 0.1142 - accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 598us/step - loss: 0.0481 - accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 643us/step - loss: 0.1442 - accuracy: 0.9615\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 713us/step - loss: 0.0750 - accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 626us/step - loss: 0.0452 - accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 699us/step - loss: 0.1664 - accuracy: 0.9615\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 711us/step - loss: 0.0315 - accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 784us/step - loss: 0.0986 - accuracy: 0.9615\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 637us/step - loss: 0.0214 - accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 662us/step - loss: 0.0597 - accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 763us/step - loss: 0.0653 - accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 678us/step - loss: 0.1453 - accuracy: 0.9231\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 670us/step - loss: 0.0290 - accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 692us/step - loss: 0.0481 - accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 624us/step - loss: 0.0436 - accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 728us/step - loss: 0.1014 - accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 651us/step - loss: 0.0517 - accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 687us/step - loss: 0.0404 - accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 683us/step - loss: 0.0768 - accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 799us/step - loss: 0.0839 - accuracy: 0.9615\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 624us/step - loss: 0.0947 - accuracy: 0.9615\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 755us/step - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 676us/step - loss: 0.0908 - accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 642us/step - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 644us/step - loss: 0.0817 - accuracy: 0.9615\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 671us/step - loss: 0.0230 - accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 718us/step - loss: 0.1144 - accuracy: 0.9615\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 723us/step - loss: 0.0399 - accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 727us/step - loss: 0.0365 - accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 635us/step - loss: 0.0337 - accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 676us/step - loss: 0.0354 - accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 642us/step - loss: 0.0282 - accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 608us/step - loss: 0.0491 - accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 658us/step - loss: 0.0195 - accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 962us/step - loss: 0.0452 - accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 702us/step - loss: 0.0351 - accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 719us/step - loss: 0.0322 - accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 638us/step - loss: 0.0231 - accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 668us/step - loss: 0.0226 - accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 657us/step - loss: 0.0386 - accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 597us/step - loss: 0.0490 - accuracy: 0.9615\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 635us/step - loss: 0.0451 - accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 634us/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 600us/step - loss: 0.0310 - accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 614us/step - loss: 0.0347 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 665us/step - loss: 0.0272 - accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 633us/step - loss: 0.0310 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 608us/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 637us/step - loss: 0.0515 - accuracy: 0.9615\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 631us/step - loss: 0.0304 - accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 652us/step - loss: 0.0350 - accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 621us/step - loss: 0.0323 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 810us/step - loss: 0.0304 - accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 737us/step - loss: 0.0311 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 712us/step - loss: 0.1034 - accuracy: 0.9615\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 715us/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 638us/step - loss: 0.0769 - accuracy: 0.9615\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 654us/step - loss: 0.0251 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 750us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 668us/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 635us/step - loss: 0.0212 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 657us/step - loss: 0.0233 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 650us/step - loss: 0.0300 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 649us/step - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 614us/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 639us/step - loss: 0.0366 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 724us/step - loss: 0.0087 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 630us/step - loss: 0.0274 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 646us/step - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 682us/step - loss: 0.0324 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 659us/step - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 667us/step - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 601us/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 651us/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 712us/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0640 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 648us/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 665us/step - loss: 0.0631 - accuracy: 0.9615\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 695us/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 576us/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 664us/step - loss: 0.0386 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 625us/step - loss: 0.0776 - accuracy: 0.9615\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 659us/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 701us/step - loss: 0.0233 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 634us/step - loss: 0.0287 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 713us/step - loss: 0.0690 - accuracy: 0.9615\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 604us/step - loss: 0.0191 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 715us/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 663us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 548us/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 598us/step - loss: 0.0306 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 615us/step - loss: 0.0310 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 663us/step - loss: 0.0212 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 659us/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 588us/step - loss: 0.0444 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 664us/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 633us/step - loss: 0.0427 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 621us/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 627us/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 661us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 611us/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 688us/step - loss: 0.0248 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 727us/step - loss: 0.0174 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 635us/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 609us/step - loss: 0.0343 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 687us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 614us/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 650us/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 629us/step - loss: 0.0198 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 708us/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 668us/step - loss: 0.0492 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 633us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 621us/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 651us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 686us/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 610us/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 697us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 585us/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 640us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 621us/step - loss: 0.0242 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 604us/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 641us/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 678us/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 627us/step - loss: 0.0641 - accuracy: 0.9615\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 651us/step - loss: 0.0921 - accuracy: 0.9615\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 684us/step - loss: 0.0639 - accuracy: 0.9615\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 727us/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 760us/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 698us/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 634us/step - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 632us/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 635us/step - loss: 0.0315 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 579us/step - loss: 0.0394 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 591us/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 678us/step - loss: 0.0241 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 641us/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 645us/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 713us/step - loss: 0.0156 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 829us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 710us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 780us/step - loss: 0.0261 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 707us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 813us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 740us/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 712us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 696us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0037 - accuracy: 1.00 - 0s 755us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 0s 766us/step - loss: 0.0291 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 0s 664us/step - loss: 0.0294 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 0s 715us/step - loss: 0.0163 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 0s 679us/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s 861us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s 700us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "model is created\n"
     ]
    }
   ],
   "source": [
    "# deep neural networds model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(53,), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "#Training and saving the model \n",
    "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
    "\n",
    "model.save('data/chatbot_model.h5', hist)\n",
    "print(\"model is created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6a731766ec59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/cpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/chatbot_model.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mintents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/intents.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/words.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/classes.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    model = load_model('data/chatbot_model.h5', )\n",
    "intents = json.loads(open('data/intents.json').read())\n",
    "words = pickle.load(open('data/words.pkl','rb'))\n",
    "classes = pickle.load(open('data/classes.pkl','rb'))\n",
    "\n",
    "\n",
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern - splitting words into array\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stemming every word - reducing to base form\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "\n",
    "# return bag of words array: 0 or 1 for words that exist in sentence\n",
    "def bag_of_words(sentence, words, show_details=True):\n",
    "    # tokenizing patterns\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words - vocabulary matrix\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,word in enumerate(words):\n",
    "            if word == s: \n",
    "                # assign 1 if current word is in the vocabulary position\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % word)\n",
    "    return(np.array(bag))\n",
    "\n",
    "def predict_class(sentence):\n",
    "    # filter below  threshold predictions\n",
    "    p = bag_of_words(sentence, words,show_details=False)\n",
    "    res = model.predict(np.array([p]))[0]\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
    "    # sorting strength probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "    return return_list\n",
    "\n",
    "def getResponse(ints, intents_json):\n",
    "    tag = ints[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if(i['tag']== tag):\n",
    "            result = random.choice(i['responses'])\n",
    "            break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python_response'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg=\"How to use lists?\"\n",
    "ints = predict_class(msg)\n",
    "res = getResponse(ints, intents)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs as tfjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lasker/anaconda3/envs/.notebook_env/lib/python3.7/site-packages/tensorflowjs/converters/keras_h5_conversion.py:122: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  return h5py.File(h5file)\n"
     ]
    }
   ],
   "source": [
    "tfjs.converters.save_keras_model(model, 'chatbot_model_js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "json.dump(intents, open('chatbot_model_js/intents.json','w'))\n",
    "json.dump(words, open('chatbot_model_js/words.json','w'))\n",
    "json.dump(classes, open('chatbot_model_js/classes.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.load(open())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
